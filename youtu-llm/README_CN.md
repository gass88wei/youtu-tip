<div align="center">

# <img src="assets/logo.svg" alt="Tencent Youtu Lab Logo" height="26px"> Youtu-LLM: <br>è§£é”è½»é‡çº§å¤§è¯­è¨€æ¨¡å‹çš„åŸç”Ÿæ™ºèƒ½ä½“æ½œåŠ›

[ğŸ”– English](README.md) â€¢ [ğŸ¤— æ¨¡å‹](https://huggingface.co/collections/tencent/youtu) â€¢ [ğŸ“‘ æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/2512.24618) â€¢ [â­ è´¡çŒ®ä¸åˆ›æ–°](#contributions) â€¢ [ğŸ“Š æ€§èƒ½å¯¹æ¯”](#benchmarks) â€¢ [ğŸš€ å¿«é€Ÿå…¥é—¨](#quickstart)

</div>

## ğŸ¯ ç®€ä»‹

**Youtu-LLM**æ˜¯ä¸€æ¬¾å…¨æ–°ã€å°å·§ä½†å¼ºå¤§çš„LLMï¼Œä»…åŒ…å«1.96Bå‚æ•°ï¼Œæ”¯æŒ128Kä¸Šä¸‹æ–‡ï¼Œå¹¶å…·å¤‡åŸç”Ÿæ™ºèƒ½ä½“èƒ½åŠ›ã€‚åœ¨é€šç”¨è¯„ä¼°ä¸­ï¼ŒYoutu-LLMåœ¨å¸¸è¯†ã€STEMã€ä»£ç å’Œé•¿æ–‡èƒ½åŠ›ä¸Šæ˜¾è‘—ä¼˜äºåŒç­‰è§„æ¨¡çš„ç°æœ‰LLMï¼›åœ¨æ™ºèƒ½ä½“ç›¸å…³æµ‹è¯•ä¸­ï¼ŒYoutu-LLMè¶…è¶Šäº†è§„æ¨¡æ›´å¤§çš„é¢†å…ˆè€…ï¼Œå¹¶çœŸæ­£èƒ½å¤Ÿå®Œæˆå¤šä¸ªç«¯åˆ°ç«¯çš„æ™ºèƒ½ä½“ä»»åŠ¡ã€‚

**Youtu-LLM**å…·æœ‰ä»¥ä¸‹ç‰¹æ€§ï¼š
- ç±»å‹: åŸºäºå¯†é›†[MLA](https://arxiv.org/abs/2405.04434)çš„è‡ªå›å½’LLM
- å‘å¸ƒç‰ˆæœ¬: [Base](https://huggingface.co/tencent/Youtu-LLM-2B-Base)å’Œ[Instruct](https://huggingface.co/tencent/Youtu-LLM-2B)
- æ€»å‚æ•°é‡: 1.96B
- å±‚æ•°: 32
- æ³¨æ„åŠ›å¤´æ•°ï¼ˆMLAï¼‰: 16 for Q/K/V
- MLA Rank: 1536 for Q, 512 for K/V 
- MLAç»´åº¦: 128 for QK Nope, 64 for QK Rope, and 128 for V
- æ”¯æŒæ–‡æœ¬é•¿åº¦: 131072
- è¯è¡¨å¤§å°: 128256

<a id="contributions"></a>

## ğŸš€ è´¡çŒ®ä¸åˆ›æ–°

Youtu-LLMçš„ä¸»è¦è´¡çŒ®å¦‚ä¸‹:
- ğŸ¯ **ä»¥STEMèƒ½åŠ›ä¸ºå‡ºå‘ç‚¹çš„è®¾è®¡**ï¼šYoutu-LLMçš„è®¾è®¡ä»¥STEMèƒ½åŠ›å’Œæ™ºèƒ½ä½“èƒ½åŠ›ä¸ºå‡ºå‘ç‚¹ï¼Œæ¶‰åŠè¯è¡¨æ„å»ºã€æ•°æ®é…æ¯”å’Œå¤šé˜¶æ®µè¯¾ç¨‹å­¦ä¹ ç­–ç•¥ã€‚
- ğŸ’¡ **åŸç”Ÿæ™ºèƒ½ä½“èƒ½åŠ›**ï¼šYoutu-LLMä½¿ç”¨128Kä¸Šä¸‹æ–‡è¿›è¡ŒåŸç”Ÿè®­ç»ƒï¼Œå¹¶è¾…ä»¥æ™ºèƒ½ä½“ä¸­æœŸè®­ç»ƒï¼ˆAgentic Mid-trainingï¼‰ï¼Œä»è€Œèƒ½å¤Ÿåœ¨ç«¯ä¾§åœºæ™¯ä¸­å®ç°æ›´å¤šè½®æ¬¡çš„äº¤äº’ã€‚
- âš¡ **SOTA æ€§èƒ½**ï¼šYoutu-LLMåŸºäºdense MLAæ¶æ„ï¼Œåœ¨è½»é‡çº§LLMä¸Šå®ç°äº†SOTAæ€§èƒ½ï¼Œè¶…è¶Šäº†ä¼ ç»Ÿçš„dense GQA/MHAèŒƒå¼ã€‚MLA æ¶æ„ä¹Ÿæ„å‘³ç€Youtu-LLMå¯ä»¥è½»æ¾é›†æˆåˆ°ç°æœ‰çš„é¢å‘DSV3çš„ç”Ÿæ€ç³»ç»Ÿä¸­ã€‚

<a id="benchmarks"></a>

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### åŸºç¡€æ¨¡å‹
#### é€šç”¨åŸºå‡†æµ‹è¯•
| Type | Benchmark (Metric) | # Shots | Qwen3-1.7B-Base | SmoLM3-3B-Base | Gemma3-4B-Base | Qwen3-4B-Base | Llama3.1-8B | Youtu-LLM-2B-Base |
| :--- | :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Commonsense  | MMLU-Pro (EM) | 5 | 34.9% | 35.3% | 29.4% | <u>46.1%</u> | 36.2% | **48.4%** |
|              | MLQA-Zh (EM) | 3 | 38.1% | 38.0% | 40.3% | **47.2%** | 43.0% | <u>43.5%</u> |
|              | MMLU-ProX-Zh (EM) | 5 | 32.5% | 26.7% | 24.2% | **45.2%** | 25.4% | <u>40.7%</u> |
| STEM         | GSM8K (EM) | 8 | 68.2% | 67.3% | 38.5% | **80.8%** | 47.8% | <u>77.6%</u> |
|              | MGSM-Zh (EM) | 8 | 57.1% | 40.7% | 33.0% | **69.7%** | 35.9% | <u>68.9%</u> |
|              | MATH (EM) | 4 | 28.1% | 40.8% | 24.4% | **44.8%** | 21.5% | <u>44.4%</u> |
|              | BBH (EM) | 3 | 53.0% | 59.8% | 51.6% | **70.8%** | <u>62.9%</u> | 59.8% |
|              | GPQA-MC (Acc. Norm) | 5 | 30.4% | 26.6% | 28.6% | **37.8%** | 30.1% | <u>33.3%</u> |
|              | HLE-MC (Acc. Norm) | 3 | 10.7% | 3.1% | 8.0% | <u>15.0%</u> | 11.5% | **17.4%** |
| Coding       | MBPP (Pass@1) | 3 | 55.6% | 51.0% | 45.8% | **67.5%** | 49.4% | <u>66.6%</u> |
|              | MBPP+ (Pass@1) | 3 | 71.0% | 66.1% | 61.9% | <u>80.8%</u> | 62.7% | **81.8%** |
|              | HumanEval (Pass@1) | 0 | 49.9% | 34.8% | 36.6% | <u>57.6%</u> | 36.0% | **64.6%** |
|              | HumanEval+ (Pass@1) | 0 | 41.3% | 28.1% | 28.1% | <u>49.9%</u> | 28.1% | **57.3%** |
|              | LiveCodeBench v6 (Pass@1) | 3 | 5.1% | 2.9% | 2.9% | <u>6.9%</u> | 3.4% | **9.7%** |
|              | CRUXEval (Pass@1) | 1 | 40.6% | 42.1% | 39.7% | <u>54.8%</u> | 42.3% | **55.9%** |
|              | RepoBench (EM) | 3 | 21.0% | 21.8% | 23.0% | **25.3%** | <u>25.2%</u> | 22.7% |
| Long Context | LongBench v2 (Acc.) | 3 | <u>28.0%</u> | **28.8%** | 26.6% | 25.8% | 27.8% | 27.2% |
|              | NIAH (Acc.) | / | 79.8% | 75.0% | <u>99.5%</u> | 83.0% | **99.8%** | 98.8% |

#### æ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•
æˆ‘ä»¬ä½¿ç”¨[APTBench](https://github.com/TencentYoutuResearch/APTBench/)æ¥è¯„ä¼°åŸºç¡€æ¨¡å‹çš„æ™ºèƒ½ä½“èƒ½åŠ›ã€‚

| Category | Qwen3-1.7B-Base | SmoLM3-3B-Base | Gemma3-4B-Base | Qwen3-4B-Base | Llama3.1-8B | Youtu-LLM-2B-Base |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
| Code | 25.1% | 24.3% | 32.8% | **41.9%** | 23.6% | <u>37.9%</u> |
| Deep Research | 28.5% | 27.2% | 36.4% | **40.5%** | 30.0% | <u>38.6%</u> |
| Math | 59.9% | 60.7% | 59.8% | **70.5%** | 60.1% | <u>68.0%</u> |
| Tool | 56.7% | 59.1% | 61.7% | **65.8%** | 64.1% | <u>64.2%</u> |

### æŒ‡ä»¤æ¨¡å‹
#### é€šç”¨åŸºå‡†æµ‹è¯•
| Benchmark | DeepSeek-R1-Distill-Qwen-1.5B | Qwen3-1.7B | SmolLM3-3B | Qwen3-4B | DeepSeek-R1-Distill-Llama-8B | Youtu-LLM-2B |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: |
| **Commonsense Knowledge Reasoning** | | | | | | |
| MMLU-Redux | 53.0% | 74.1% | 75.6% | **83.8%** | <u>78.1%</u> | 75.8% |
| MMLU-Pro | 36.5% | 54.9% | 53.0% | **69.1%** | 57.5% | <u>61.6%</u> |
| **Instruction Following & Text Reasoning** | | | | | | |
| IFEval | 29.4% | 70.4% | 60.4% | **83.6%** | 34.6% | <u>81.2%</u> |
| DROP | 41.3% | 72.5% | 72.0% | <u>82.9%<u> | 73.1% | **86.7%** |
| MUSR | 43.8% | 56.6% | 54.1% | **60.5%** | <u>59.7%</u> | 57.4% |
| **STEM** | | | | | | |
| MATH-500 | 84.8% | 89.8% | 91.8% | **95.0%** | 90.8% | <u>93.7%</u> |
| AIME 24 | 30.2% | 44.2% | 46.7% | **73.3%** | 52.5% | <u>65.4%</u> |
| AIME 25 | 23.1% | 37.1% | 34.2% | **64.2%** | 34.4% | <u>49.8%</u> |
| GPQA-Diamond | 33.6% | 36.9% | 43.8% | **55.2%** | 45.5% | <u>48.0%</u> |
| BBH | 31.0% | 69.1% | 76.3% | **87.8%** | <u>77.8%</u> | 77.5% |
| **Coding** | | | | | | |
| HumanEval | 64.0% | 84.8% | 79.9% | <u>95.4%<u> | 88.1% | **95.9%** |
| HumanEval+ | 59.5% | 76.2% | 74.7% | <u>87.8%</u> | 82.5% | **89.0%** |
| MBPP | 51.5% | 80.5% | 66.7% | **92.3%** | 73.9% | <u>85.0%</u> |
| MBPP+ | 44.2% | 67.7% | 56.7% | **77.6%** | 61.0% | <u>71.7%</u> |
| LiveCodeBench v6 | 19.8% | 30.7% | 30.8% | **48.5%** | 36.8% | <u>43.7%</u> |

#### æ™ºèƒ½ä½“åŸºå‡†æµ‹è¯•
| Benchmark | Qwen3-1.7B | SmolLM3-3B | Qwen3-4B | Youtu-LLM-2B |
| :--- | :---: | :---: | :---: | :---: |
| **Deep Research** | | | | |
| GAIA | 11.4% | 11.7% | <u>25.5%</u> | **33.9%** |
| xbench | 11.7% | 13.9% | <u>18.4%</u> | **19.5%** |
| **Code** | | | | |
| SWE-Bench-Verified | 0.6% | <u>7.2%</u> | 5.7% | **17.7%** |
| EnConda-Bench | 10.8% | 3.5% | <u>16.1%</u> | **21.5%** |
| **Tool** | | | | |
| BFCL V3 | 55.5% | 31.5% | **61.7%** | <u>58.0%</u> |
| Ï„Â²-Bench | 2.6% | 9.7% | <u>10.9%</u> | **15.0%** |

## ğŸ“ è¯„ä¼°å¤ç°

æˆ‘ä»¬æä¾›äº†ç”¨äºå¤ç°ä¸Šè¿°åˆ†æ•°çš„è¯„ä¼°ä»£ç ã€‚
- å¯¹äº[Youtu-LLM-2B-Base](https://huggingface.co/tencent/Youtu-LLM-2B-Base)ï¼Œæ‰€æœ‰çŸ­æ–‡é€šç”¨åŸºå‡†æµ‹è¯•å¯ä½¿ç”¨[base_eval](base_eval/)è¿›è¡Œè¯„ä¼°ï¼Œæ™ºèƒ½ä½“æŒ‡æ ‡å¯ä½¿ç”¨[APTBench](https://github.com/TencentYoutuResearch/APTBench/)è·å–ã€‚
- å¯¹äº[Youtu-LLM-2B](https://huggingface.co/tencent/Youtu-LLM-2B)ï¼Œæ‰€æœ‰é€šç”¨åŸºå‡†æµ‹è¯•å¯ä½¿ç”¨[instruct_eval](instruct_eval/)è¿›è¡Œè¯„ä¼°ã€‚

<a id="quickstart"></a>

## ğŸš€ å¿«é€Ÿå…¥é—¨

æœ¬æŒ‡å—å°†å¸®åŠ©æ‚¨å¿«é€Ÿéƒ¨ç½²å¹¶è°ƒç”¨ **Youtu-LLM-2B** æ¨¡å‹ã€‚è¯¥æ¨¡å‹æ”¯æŒâ€œæ€è€ƒæ¨¡å¼â€ï¼ˆReasoning Modeï¼‰ï¼Œèƒ½å¤Ÿé€šè¿‡æ€ç»´é“¾ï¼ˆCoTï¼‰ç”Ÿæˆæ›´é«˜è´¨é‡çš„å›ç­”ã€‚

### 1. ç¯å¢ƒå‡†å¤‡

ç¡®ä¿æ‚¨çš„ Python ç¯å¢ƒå·²å®‰è£… `transformers` åº“ï¼Œä¸”ç‰ˆæœ¬ç¬¦åˆè¦æ±‚ã€‚

```bash
pip install "transformers>=4.56" torch accelerate

```

---

### 2. æ ¸å¿ƒä»£ç ç¤ºä¾‹

ä»¥ä¸‹ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åŠ è½½æ¨¡å‹ã€å¯ç”¨æ€è€ƒæ¨¡å¼ï¼Œå¹¶åˆ©ç”¨ `re` æ¨¡å—è§£æè¾“å‡ºä¸­çš„â€œæ€è€ƒè¿‡ç¨‹â€ä¸â€œæœ€ç»ˆç»“è®ºâ€ã€‚

```python
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# 1. é…ç½®æ¨¡å‹
model_id = "tencent/Youtu-LLM-2B"

# 2. åˆå§‹åŒ– Tokenizer å’Œæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    trust_remote_code=True
)

# 3. æ„å»ºå¯¹è¯è¾“å…¥
prompt = "æ‚¨å¥½"
messages = [{"role": "user", "content": prompt}]

# ä½¿ç”¨ apply_chat_template æ„é€ è¾“å…¥ï¼Œenable_thinking=True å¼€å¯æ€è€ƒæ¨¡å¼
input_ids = tokenizer.apply_chat_template(
    messages, 
    tokenize=True, 
    add_generation_prompt=True, 
    return_tensors="pt",
    enable_thinking=True
).to(model.device)

# 4. ç”Ÿæˆå›å¤
outputs = model.generate(
    input_ids,
    max_new_tokens=512,
    do_sample=True,
    temperature=1.0,
    top_p=0.95,
    repetition_penalty=1.05
)

# 5. è§£æç»“æœ
full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)

def parse_reasoning(text):
    """æå– <think> æ ‡ç­¾å†…çš„æ€è€ƒè¿‡ç¨‹ä¸ä¹‹åçš„å›ç­”å†…å®¹"""
    thought_pattern = r"<think>(.*?)</think>"
    match = re.search(thought_pattern, text, re.DOTALL)
    
    if match:
        thought = match.group(1).strip()
        answer = text.split("</think>")[-1].strip()
    else:
        thought = "ï¼ˆæœªäº§ç”Ÿæ˜¾å¼æ€è€ƒè¿‡ç¨‹ï¼‰"
        answer = text
    return thought, answer

thought, final_answer = parse_reasoning(full_response)

print(f"\n{'='*20} æ€è€ƒè¿‡ç¨‹ {'='*20}\n{thought}")
print(f"\n{'='*20} æœ€ç»ˆå›ç­” {'='*20}\n{final_answer}")

```

---

### 3. å…³é”®é…ç½®è¯´æ˜

#### æ€è€ƒæ¨¡å¼å¼€å…³

åœ¨ `apply_chat_template` æ–¹æ³•ä¸­ï¼Œé€šè¿‡ `enable_thinking` å‚æ•°æ§åˆ¶ï¼š

* **True (é»˜è®¤å»ºè®®)**ï¼šæ¿€æ´»æ€ç»´é“¾ï¼Œé€‚åˆå¤æ‚é€»è¾‘æ¨ç†ã€‚
* **False**ï¼šç›´æ¥è¾“å‡ºç»“æœï¼Œå“åº”é€Ÿåº¦æ›´å¿«ï¼Œé€‚åˆç®€å•å¯¹è¯ã€‚

#### æ¨èè§£ç å‚æ•°

æ ¹æ®ä½¿ç”¨åœºæ™¯ï¼Œå»ºè®®è°ƒæ•´ä»¥ä¸‹è¶…å‚æ•°ä»¥è·å¾—æœ€ä½³ç”Ÿæˆæ•ˆæœï¼š

| å‚æ•° | æ€è€ƒæ¨¡å¼ (Reasoning) | éæ€è€ƒæ¨¡å¼ (Normal) |
| --- | --- | --- |
| `do_sample` | `True` | `True` |
| `temperature` | **1.0** (ä¿æŒåˆ›é€ åŠ›) | **0.7** (ç»“æœæ›´ç¨³å®š) |
| `top_p` | 0.95 | 0.8 |
| `top_k` | 20 | 20 |
| `repetition_penalty` | 1.05 | - |

> **æç¤º**ï¼šåœ¨ä½¿ç”¨æ€è€ƒæ¨¡å¼æ—¶ï¼Œè¾ƒé«˜çš„ `temperature` æœ‰åŠ©äºæ¨¡å‹è¿›è¡Œæ›´æ·±å±‚çš„å‘æ•£æ€§æ€è€ƒã€‚

---

### 4. vLLM éƒ¨ç½²

æˆ‘ä»¬æä¾›ä½¿ç”¨ **vLLM 0.10.2** éƒ¨ç½²æ¨¡å‹æœåŠ¡çš„æ–¹æ³•ã€‚æ¨èä½¿ç”¨çš„ Docker é•œåƒä¸º `vllm/vllm-openai:v0.10.2`ã€‚

#### é›†æˆæ­¥éª¤
é¦–å…ˆï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤å°†Youtu-LLMæ¨¡å‹æ–‡ä»¶é›†æˆåˆ° vLLM æ¡†æ¶ä¸­ã€‚
*æ³¨æ„ï¼šè¯·å…ˆè§£å‹æˆ‘ä»¬æä¾›çš„ç»è¿‡è°ƒæ•´çš„[vllmå‹ç¼©æ–‡ä»¶](vllm_deploy/modified_vllm.zip)ï¼Œæ¥ç€å°† `<local_modified_vllm_path>` æ›¿æ¢ä¸ºåˆšåˆšè§£å‹çš„vllmä»£ç è·¯å¾„ï¼Œå°† `<vllm_path>` æ›¿æ¢ä¸º vLLM çš„å®‰è£…è·¯å¾„ã€‚*

```bash
cp <local_modified_vllm_path>/0_10_2_official/youtu_llm.py <vllm_path>/vllm/model_executor/models/youtu_llm.py
cp <local_modified_vllm_path>/0_10_2_official/configuration_youtu.py <vllm_path>/vllm/model_executor/models/configuration_youtu.py
cp <local_modified_vllm_path>/0_10_2_official/__init__.py <vllm_path>/vllm/config/__init__.py
cp <local_modified_vllm_path>/0_10_2_official/registry.py <vllm_path>/vllm/model_executor/models/registry.py
```

#### å¯åŠ¨æœåŠ¡
é›†æˆå®Œæˆåï¼Œå³å¯ä½¿ç”¨ vLLM éƒ¨ç½²æ¨¡å‹ï¼Œå¯åŠ¨å‘½ä»¤å¦‚ä¸‹ï¼š

```bash
vllm serve <model_path> --trust-remote-code
```

**å·¥å…·è°ƒç”¨ (Tool Call) æ”¯æŒï¼š**
å¦‚æœè¦ä½¿ç”¨ tool_call èƒ½åŠ›ï¼Œè¯·åœ¨å¯åŠ¨å‘½ä»¤ä¸­å¢åŠ ä»¥ä¸‹å‚æ•°ï¼š

```bash
--enable-auto-tool-choice --tool-call-parser hermes
```

## ğŸ“š Citation

å¦‚æœæœ¬å·¥ä½œå¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œå¸Œæœ›æ‚¨å¼•ç”¨æˆ‘ä»¬çš„æ–‡ç« :

```bibtex
@article{youtu-llm,
  title={Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models},
  author={Tencent Youtu Lab},
  year={2025},
  eprint={2512.24618},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2512.24618}, 
}
```
